{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-RdQuRek3tU"
   },
   "source": [
    "# Web parsing with Python, Beautiful Soup and Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A3FR5YZk-z7"
   },
   "source": [
    "### 1. Get any HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g957Xy3dRe0N"
   },
   "source": [
    "It's very easy to extract the source code of a web page in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbWU5oBkngSN"
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUegttKrofLv"
   },
   "outputs": [],
   "source": [
    "# a very lightweight website\r\n",
    "url = 'https://lite.cnn.com/en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's render it here (I love Jupyter)\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=url, width='100%', height='250ps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHCP3SZlolEx"
   },
   "outputs": [],
   "source": [
    "answer = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImPFUxLMolI3"
   },
   "outputs": [],
   "source": [
    "# what could we do with an answer\r\n",
    "print(answer.url)\r\n",
    "print(answer.status_code)\r\n",
    "print(answer.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3J90_p-olNV"
   },
   "outputs": [],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiwDsQCgSJkz"
   },
   "source": [
    "**That looks like a lot of things. We have to somehow navigate through HTML**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0uGENIXk_Gk"
   },
   "source": [
    "### 2. Use BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eAsZmUrnj3M"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxMfEvmZpM5K"
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(answer.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCXpYS6ipNEg"
   },
   "outputs": [],
   "source": [
    "# now we can recognize some structure\r\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eImaF0N5wRkA"
   },
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVJrdKdspNF1"
   },
   "outputs": [],
   "source": [
    "# let's find the links\r\n",
    "soup.find_all('a')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7w-JDOPteKr"
   },
   "outputs": [],
   "source": [
    "# and get the title of one\r\n",
    "soup.find_all('a')[5].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bk67-BsoVmll"
   },
   "source": [
    "Now to serious business!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_y4R6ai1k_RU"
   },
   "source": [
    "### 3. Scrape Airbnb page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0E8PDiRVvdE"
   },
   "source": [
    "Let's get to the website and look for some apartments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQaQOXZQnm7j"
   },
   "outputs": [],
   "source": [
    "# Let's plan a trip to Austrian Alps\r\n",
    "airbnb_url = 'https://www.airbnb.com/s/Mayrhofen--Austria/homes?tab_id=home_tab&refinement_paths%5B%5D=%2Fhomes&date_picker_type=calendar&query=Mayrhofen%2C%20Austria&place_id=ChIJbzLYLzjdd0cRDtGuTzM_vt4&checkin=2021-02-06&checkout=2021-02-13&adults=4&source=structured_search_input_header&search_type=autocomplete_click'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mN68-cGPuIWn"
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(requests.get(airbnb_url).content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IG7YjnIDuIZf"
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_LlzLBennNL"
   },
   "source": [
    "### 4. Inspect elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_BJoRdOw5Mt"
   },
   "source": [
    "Press F12 ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o87Q3B2Ik_iU"
   },
   "source": [
    "### 5. Scrape 1 element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8giGyc5LntCe"
   },
   "outputs": [],
   "source": [
    "soup.find_all('div', '_gig1e7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGN6ju9mxfeT"
   },
   "outputs": [],
   "source": [
    "# we can also extract its child tag\r\n",
    "soup.find_all('div', '_8s3ctt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WLbqzHsxfjF"
   },
   "outputs": [],
   "source": [
    "listings = soup.find_all('div', '_8s3ctt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4eSngGcxfox"
   },
   "outputs": [],
   "source": [
    "listings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PRgYZ7dxfus"
   },
   "outputs": [],
   "source": [
    "listings[0].find_all('a')[0].get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwLVy8DD0IHO"
   },
   "outputs": [],
   "source": [
    "listings[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RQ2DGq-0cua"
   },
   "source": [
    "### 6. Inspect all data elements on search page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hCpi1GLgFZH"
   },
   "source": [
    "**smithio.medium.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyFldkFv_ulk"
   },
   "source": [
    "<img src='https://miro.medium.com/max/700/1*GLNHp0QOf5qZiHa1bnaRvg.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8jQrsh8_-Lv"
   },
   "outputs": [],
   "source": [
    "# url: tag=a, get=href\r\n",
    "# name: tag=div, class=_hxt6u1e, get=aria-label\r\n",
    "# header: tag=div, class=_b14dlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATnJssp40e6C"
   },
   "source": [
    "### 7. Write a scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13W65EWn0gMD"
   },
   "outputs": [],
   "source": [
    "# First Generation :)\r\n",
    "def extract_basic_features(listing_html):\r\n",
    "    features_dict = {}\r\n",
    "    \r\n",
    "    url = listing_html.find('a').get('href')\r\n",
    "    name = listing_html.find(\"div\", {\"class\": \"_hxt6u1e\"}).get('aria-label')\r\n",
    "    header = listing_html.find(\"div\", {\"class\": \"_b14dlit\"}).get_text()\r\n",
    "    \r\n",
    "    features_dict['url'] = url\r\n",
    "    features_dict['name'] = name\r\n",
    "    features_dict['header'] = header\r\n",
    "    \r\n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Hzy5KCoBWq8"
   },
   "outputs": [],
   "source": [
    "extract_basic_features(listings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlgLdUwmDuVA"
   },
   "outputs": [],
   "source": [
    "# what if the tag is not found?\r\n",
    "listings[0].find('b').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAEpNHEEBW2h"
   },
   "outputs": [],
   "source": [
    "# Second Generation :)\r\n",
    "def extract_basic_features(listing_html):\r\n",
    "    features_dict = {}\r\n",
    "    \r\n",
    "    try:\r\n",
    "        url = listing_html.find('b').get('href')\r\n",
    "    except:\r\n",
    "        url = 'empty'\r\n",
    "    try:\r\n",
    "        name = listing_html.find(\"div\", {\"class\": \"_hxt6u1e\"}).get('aria-label')\r\n",
    "    except:\r\n",
    "        name = 'empty'\r\n",
    "    try:\r\n",
    "        header = listing_html.find(\"div\", {\"class\": \"_b14dlit\"}).text\r\n",
    "    except:\r\n",
    "        header = 'empty'\r\n",
    "    \r\n",
    "    \r\n",
    "    features_dict['url'] = url\r\n",
    "    features_dict['name'] = name\r\n",
    "    features_dict['header'] = header\r\n",
    "    \r\n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EiVinF3h1JZ"
   },
   "outputs": [],
   "source": [
    "extract_basic_features(listings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXbEBtdwBW9G"
   },
   "outputs": [],
   "source": [
    "# too many separate extractions\r\n",
    "RULES_SEARCH_PAGE = {\r\n",
    "    'url': {'tag': 'a', 'get': 'href'},\r\n",
    "    'name': {'tag': 'div', 'class': '_hxt6u1e', 'get': 'aria-label'},\r\n",
    "    'header': {'tag': 'div', 'class': '_b14dlit'},\r\n",
    "    'rooms': {'tag': 'div', 'class': '_kqh46o'},\r\n",
    "    'facilities': {'tag': 'div', 'class': '_kqh46o', 'order': 1},\r\n",
    "    'badge': {'tag': 'div', 'class': '_17bkx6k'},\r\n",
    "    'rating_n_reviews': {'tag': 'span', 'class': '_18khxk1'},\r\n",
    "    'price': {'tag': 'span', 'class': '_1p7iugi'},\r\n",
    "    'superhost': {'tag': 'div', 'class': '_ufoy4t'},\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1b2udbwEeE6"
   },
   "outputs": [],
   "source": [
    "# Third Generation :)\r\n",
    "def extract_element(listing_html, params):\r\n",
    "    # 1. Find the right tag\r\n",
    "    if 'class' in params:\r\n",
    "        elements_found = listing_html.find_all(params['tag'], params['class'])\r\n",
    "    else:\r\n",
    "        elements_found = listing_html.find_all(params['tag'])\r\n",
    "\r\n",
    "    # 2. Extract the right element\r\n",
    "    tag_order = params.get('order', 0)\r\n",
    "    element = elements_found[tag_order]\r\n",
    "        \r\n",
    "    # 3. Get text\r\n",
    "    if 'get' in params:\r\n",
    "        output = element.get(params['get'])\r\n",
    "    else:\r\n",
    "        output = element.get_text()\r\n",
    "\r\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqnXvfumFge2"
   },
   "outputs": [],
   "source": [
    "print(extract_element(listings[0], RULES_SEARCH_PAGE['header']))\r\n",
    "print(extract_element(listings[0], RULES_SEARCH_PAGE['url']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRZLtB2HHFxo"
   },
   "outputs": [],
   "source": [
    "for feature in RULES_SEARCH_PAGE:\r\n",
    "    print(f\"{feature}: {extract_element(listings[0], RULES_SEARCH_PAGE[feature])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbP1s6nJJySK"
   },
   "outputs": [],
   "source": [
    "for feature in RULES_SEARCH_PAGE:\r\n",
    "    try:\r\n",
    "        print(f\"{feature}: {extract_element(listings[0], RULES_SEARCH_PAGE[feature])}\")\r\n",
    "    except:\r\n",
    "        print(f\"{feature}: empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUxAq14wjecI"
   },
   "source": [
    "YAY!!! We're extracted all the features from one listing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T1QCX-U0ggn"
   },
   "source": [
    "### 8. Explore pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hPt1U_RKfA9"
   },
   "source": [
    "<img src='https://miro.medium.com/max/564/1*Q9iBSu5nniBwc8Wt2-8Ujw.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gqTJoYy0h6g"
   },
   "outputs": [],
   "source": [
    "airbnb_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yo6OYalILIn8"
   },
   "outputs": [],
   "source": [
    "# let's finally write this function\r\n",
    "def get_listings(search_page):\r\n",
    "    soup = BeautifulSoup(requests.get(search_page).content, 'html.parser')\r\n",
    "    listings = soup.find_all('div', '_8s3ctt')\r\n",
    "\r\n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TqYmtCtLIrg"
   },
   "outputs": [],
   "source": [
    "# it works\r\n",
    "len(get_listings(airbnb_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLMKciXALcm1"
   },
   "outputs": [],
   "source": [
    "# let's try next page\r\n",
    "new_url = airbnb_url + '&items_offset=20'\r\n",
    "len(get_listings(new_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "816wu53DLqm8"
   },
   "outputs": [],
   "source": [
    "# checking the content, if the data is there\r\n",
    "print(extract_element(get_listings(airbnb_url)[0], RULES_SEARCH_PAGE['name']))\r\n",
    "print(extract_element(get_listings(new_url)[0], RULES_SEARCH_PAGE['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLeff4kG0iNA"
   },
   "source": [
    "### 9. Collect all urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ynwY0LC0jTt"
   },
   "outputs": [],
   "source": [
    "# let's iterate through all 15 pages\r\n",
    "all_listings = []\r\n",
    "for i in range(15):\r\n",
    "    offset = 20 * i\r\n",
    "    new_url = airbnb_url + f'&items_offset={offset}'\r\n",
    "    new_listings = get_listings(new_url)\r\n",
    "    all_listings.extend(new_listings)\r\n",
    "    \r\n",
    "    # let's check if it's scraping\r\n",
    "    print(len(all_listings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnlQHnzVMQ71"
   },
   "outputs": [],
   "source": [
    "# why? maybe Airbnb tries to prevent scraping\r\n",
    "# let's wait a couple of seconds after every iteration\r\n",
    "import time\r\n",
    "\r\n",
    "all_listings = []\r\n",
    "for i in range(15):\r\n",
    "    offset = 20 * i\r\n",
    "    new_url = airbnb_url + f'&items_offset={offset}&section_offset=3'\r\n",
    "    new_listings = get_listings(new_url)\r\n",
    "    all_listings.extend(new_listings)\r\n",
    "    \r\n",
    "    # let's check if it's scraping\r\n",
    "    print(len(all_listings))\r\n",
    "\r\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJ8YcdpCN11l"
   },
   "source": [
    "Not perfect but some improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pt7Rri6YMU0U"
   },
   "outputs": [],
   "source": [
    "# another random check, if the data is there\r\n",
    "print(extract_element(all_listings[113], RULES_SEARCH_PAGE['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcPbAKvJ0jng"
   },
   "source": [
    "### 10. Scrape all search pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0aizhCCOHRt"
   },
   "source": [
    "1. build all urls\r\n",
    "2. iteratively scrape them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSrDAI2gOLEO"
   },
   "outputs": [],
   "source": [
    "# 1. build all urls\r\n",
    "def build_urls(main_url, listings_per_page=20, pages_per_location=15):\r\n",
    "    url_list = []\r\n",
    "    for i in range(pages_per_location):\r\n",
    "        offset = listings_per_page * i\r\n",
    "        url_pagination = main_url + f'&items_offset={offset}'\r\n",
    "        url_list.append(url_pagination)\r\n",
    "    \r\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2JMis3fOLK6"
   },
   "outputs": [],
   "source": [
    "# safe function to extract all features from one page\r\n",
    "def extract_page_features(soup, rules):\r\n",
    "    features_dict = {}\r\n",
    "    for feature in rules:\r\n",
    "        try:\r\n",
    "            features_dict[feature] = extract_element(soup, rules[feature])\r\n",
    "        except:\r\n",
    "            features_dict[feature] = 'empty'\r\n",
    "    \r\n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEItC3xdOLPk"
   },
   "outputs": [],
   "source": [
    "# 2. Iteratively scrape pages\r\n",
    "def process_search_pages(url_list):\r\n",
    "    features_list = []\r\n",
    "    for page in url_list:\r\n",
    "        listings = get_listings(page)\r\n",
    "        for listing in listings:\r\n",
    "            features = extract_page_features(listing, RULES_SEARCH_PAGE)\r\n",
    "            features_list.append(features)\r\n",
    "\r\n",
    "    return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUPBTKOQOLTs"
   },
   "outputs": [],
   "source": [
    "# build a list of URLs\r\n",
    "url_list = build_urls(airbnb_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DQKrjb2mmMl"
   },
   "outputs": [],
   "source": [
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46FZhTDmmko9"
   },
   "outputs": [],
   "source": [
    "# try for one page\r\n",
    "base_features = process_search_pages(url_list[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vb_ix5wKPfjO"
   },
   "outputs": [],
   "source": [
    "base_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYeEHZ6tZPYt"
   },
   "source": [
    "### 11. Look at it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FxHvhWLIcET"
   },
   "source": [
    "https://github.com/x-technology/airbnb-analytics/blob/main/Part%201%20-%20Web%20Scraping/data_sample.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujT-A61oW_hu"
   },
   "source": [
    "# All imports in one cell (just in case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7lYe290m2rK"
   },
   "outputs": [],
   "source": [
    "# all imports\r\n",
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from selenium import webdriver\r\n",
    "from selenium.webdriver.chrome.options import Options\r\n",
    "\r\n",
    "import json\r\n",
    "import time\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "from multiprocessing import Pool\r\n",
    "\r\n",
    "import os"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "WebScraping - Session I.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
